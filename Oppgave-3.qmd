---
title: "Slutninger fra statistiske modeller og statistisk styrke"
author: "Kandidatnummer: 503"
format: 
  html:
    code-fold: true
execute:
  warning: false  # Skjuler varselmeldinger
editor_options: 
  chunk_output_type: console
bibliography: resources/ref3.bib
---

## Introduksjon

I denne oppgaven skal se på statistisk forskning. Vi skal simulere to forskningsprosjekt med forskjellig størrelse på utvalg. Den første gruppen (m1) har et utvalg på 8 målinger, og den andre gruppen (m2) har et utvalg på 40 målinger. Vi skal se hva forskjellig størrelse på utvalg gjør med resultatene.

## Simulasjon

```{r}
#| echo: false
#| warning: false
#| message: false

library(tidyverse)

set.seed(1)
population <- rnorm(1000000, mean = 1.5, sd = 3)


samp1 <- data.frame(y = sample(population, 8, replace = FALSE))

samp2 <- data.frame(y = sample(population, 40, replace = FALSE))


m1 <- lm(y ~ 1, data = samp1)
m2 <- lm(y ~ 1, data = samp2)

summary(m1)
summary(m2)


```

### Oppgave 1.

**Explain the estimate, SE, t-value, and p-value from the regression models that we created previously (m1 and m2).**

Over kan vi se resultatene av simuleringen, og resultatene presenteres i @tbl-sim-results:

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: tbl-sim-results
#| tbl-cap: "Oppsummering av modellresultater"
#| fig-pos: "H"



# Laste nødvendige pakker
library(tidyverse)
library(gt)

# Oppsummering av modellene
m1_summary <- summary(m1)
m2_summary <- summary(m2)

# Trekke ut relevante verdier fra modellene
model_results <- tibble(
  Modell = c("M1", "M2"),
  Estimat = c(m1_summary$coefficients[1, 1], m2_summary$coefficients[1, 1]),
  `Standard Feil` = c(m1_summary$coefficients[1, 2], m2_summary$coefficients[1, 2]),
  `t-verdi` = c(m1_summary$coefficients[1, 3], m2_summary$coefficients[1, 3]),
  `p-verdi` = c(m1_summary$coefficients[1, 4], m2_summary$coefficients[1, 4])
)

# Lage tabellen med gt
model_results %>%
  gt() %>%
  fmt_number(
    columns = c(Estimat, `Standard Feil`, `t-verdi`, `p-verdi`),
    decimals = 3
  ) %>%
  cols_label(
    Estimat = "Estimat",
    `Standard Feil` = "Standard Feil",
    `t-verdi` = "t-verdi",
    `p-verdi` = "p-verdi"
  ) 

```

Vi kan se at i m2, med flere deltakere kommer estimatet nærmere det forhåndsbestemte gjennomsnittet på 1,5. Samtidig ser vi at standardfeil går ned, som tyder på at det mindre sannsynlig at estimatet i m2 er feil enn i m1. T-verdien blir også høyere som viser at det kommer fram en tydligere forskjell mellom null-hypotesen og den nye hypotesen. P-verdien sier noe om hvor sannsynlig det er at forskjellen er tilfeldig eller ikke. Lav p-verdi tyder på at det er lite sjanse for tilfeldigheter i resultatet [@spiegelhalter2019]. Jeg kommer tilbake til dette i neste oppgave, men vi kan allerede se her at det blir høyere reliabilitet i en studie med mange deltakere enn i en studie med få.

### Oppgave 2.

**Discuss what contributes to the different results in the two studies (m1 and m2).**

Når vi øker størelsen på utvalget ser vi at dette påvirker resultatene. I en reel studie vil vi jo ikke vite det faktiske gjennomsnittet i en populasjon. I denne simuleringen derimot har vi bestemt at gjennomsnittet (mean) er 1.5. Når vi har et utvalg på 8 observasjoner får vi resultatet 1.84, og med 40 observasjoner får vi 1.56 som estimat. Ved å øke antall faktiske observasjoner kan vi altså gjøre et mer presist estimat av hva som er gjennomsnittet i en populasjon.

På samme måte vil størelsen på utvalget påvirke standard feilen i forsøket. Standard feil beregnes ved å dele standard avvik (SD), som i denne simuleringen er 3 på kvadratroten av antall observasjoner. I denne simuleringen blir det altså tre delt på kvadratroten av 8 i m1, og kvadratroten av 40 i m2. Som vi kan se får vi da lavere standard feil (SE) i m2 hvor utvalget er større. Standard feil er et estimat av spredingenn i en populasjon, lav standard feil tyder på lav spredning og kan derfor tolkes som at utvalget er representativt for populasjonene. Lav standard feil betyr at det er stor sansynlighet for at utvalget er representativt.

T-verdien beregnes ved å dele estimatet med standard feil og forteller oss om forskjellen i gruppene er signifikant [@spiegelhalter2019]. Videre sier Spieghalter at en t-verdi over 2 tilsvarer en p-verdi, som er et mål på forskjellen mellom innsamlet data og null-hypotesen, under 0,05 som igjen vil bety at statisitkken er signifikant. I vår simulering kan vi se at dette blir avgjørende. I simuleringen med 8 i utvalger får vi en t-verdi på 1.47 som tilsvarer en p-verdi på 0.185 altså ikke signifikant. I utvalget med 40 derimot er t-verdien 3,276 og p-verdien 0.00221 som vil si at resultatet er signifikant. I forsøket med 8 observasjoner ville det altså ha blitt gjort en type II feil, altså a avvise en korrekt alternativ hypotese fordi testresultatet støtter null-hypotesen [@spiegelhalter2019].

### Oppgave 3.

**Why do we use the shaded area in the lower and upper tail of the t-distribution (See Figure).**

Grafen viser en tosidig p-verdi for m1. Midt i grafen ser vi det estimerte gjennomsnittet. En tosidig p-verdi sier noe om hvor mange observasjoner vi kan regne med å få fra populasjonen som er like ekstreme eller mer ekstreme enn den observerte t-verdien [@spiegelhalter2019]. De blå feltene viser altså hvor mange observasjoner vi kan ha utenfor null-hypotesen uten at null-hypotesen blir motbevist.

## Many studies

```{r}
#| echo: false
#| warning: false
#| message: false

library(tidyverse)

set.seed(1)
population <- rnorm(1000000, mean = 1.5, sd = 3)

# Create data frames to store the model estimates
results_8 <- data.frame(estimate = rep(NA, 1000), 
                      se = rep(NA, 1000), 
                      pval = rep(NA, 1000), 
                      n = 8)  

results_40 <- data.frame(estimate = rep(NA, 1000), 
                      se = rep(NA, 1000), 
                      pval = rep(NA, 1000), 
                      n = 40)

# A for loop used to sample 1000 studies, each iteration (i) will draw a new sample
# from the population. 

for(i in 1:1000) {
  
  # Draw a sample 
  samp1 <- data.frame(y = sample(population, 8, replace = FALSE))
  samp2 <- data.frame(y = sample(population, 40, replace = FALSE))

  # Model the data
  m1 <- lm(y ~ 1, data = samp1)
  m2 <- lm(y ~ 1, data = samp2)
  
  # Extract values from the models
  results_8[i, 1] <- coef(summary(m1))[1, 1]
  results_8[i, 2] <- coef(summary(m1))[1, 2]
  results_8[i, 3] <- coef(summary(m1))[1, 4]

  results_40[i, 1] <- coef(summary(m2))[1, 1]
  results_40[i, 2] <- coef(summary(m2))[1, 2]
  results_40[i, 3] <- coef(summary(m2))[1, 4]
  
  
}


# Save the results in a combined data frame

results <- bind_rows(results_8, results_40)

```

### Oppgave 4.

**Calculate the standard deviation of the estimate variable, and the average of the se variable for each of the study sample sizes (8 and 40). Explain why these numbers are very similar. How can you define the Standard Error (SE) in light of these calculations?**

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: tbl-sum-est-SD
#| tbl-cap: "Oppsummering av Resultater for Estimater og Standardfeil"
#| fig-pos: "H"

library(gt)
library(dplyr)

# Beregn standardavviket til estimate og gjennomsnittet av se for hver utvalgsstørrelse
results_summary <- results %>%
  group_by(n) %>%
  summarise(
    mean = mean(estimate),
    std_estimate = sd(estimate),  # Standardavviket til estimate
    avg_se = mean(se)             # Gjennomsnittet av standardfeilen
  )

# Lag tabellen med gt
results_summary %>%
  gt() %>%
  fmt_number(
    columns = c(mean, std_estimate, avg_se),
    decimals = 3
  ) %>%
  cols_label(
    mean = "Gj.snitt av Estimater",
    std_estimate = "SD av Estimater",
    avg_se = "Gj.snitt SE"
  ) 

```


Som vi kan se i @tbl-sum-est-SD er standard avvik og gjennomsnitlig standard feil svært like. Forskjellen er 0,05 i m1 og 0,014 i m2. Grunnen til at standard avvik og standard feil er så like er på grunn av de henger sammen. Standard feil finner vi som tidligere nevn ved å dele standard avvik på kvadratroten av utvalget. Begge disse verdiene blir brukt for å lage kurvemodeller som viser p-verdi og t-verdi. Når vi får lavere standard avvik og standard feil vil kurven bli smalere og spissere, fordi estimatet kommer nærmere det faktiske gjennomsnittet. Hvis vi skulle laget en kurve ut fra utvalgene m1 og m2, ville altså m1 være bredere og rundere enn m2 som ville være smal og spiss.


### Oppgave 5.

**Create a histogram (see example code below) of the p-values from each study sample-size. How do you interpret these histograms, what do they tell you about the effect of sample size on statistical power?**

```{r}
#| echo: false
#| warning: false
#| message: false

# Example code for copy and paste

# A two facets histogram can be created with ggplot2
results %>%
  ggplot(aes(pval)) + 
  geom_histogram() +
  facet_wrap(~ n)


```

I denne oppgaven kan vi se hva jeg snakket om i forrige oppgave. Et større utvalg vil gjøre at histogrammet blir smalere og spissere. Vi samler estimatene mot gjennomsnittet. Som vi kan se fra histogrammene vil et større utvalg gi lavere p-verdi, som i dette tilfelle vil gjøre resultatet signifikant som jeg har vist tidligere i oppgaven. Statistisk styrke er sansynligheten for å korrekt forkaste nullhypotesen gitt at den nye hypotesen stemmer, har en klar sammenheng med utvalgstørrelsen [@spiegelhalter2019]. Med større utvalg vil altså styrken øke, som vi også kan se i utregningen over.

### Oppgave 6.

**Calculate the number of studies from each sample size that declare a statistical significant effect (specify a threshold for, your significance level).**

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: tbl-sig-res
#| tbl-cap: "Andel Signifikante Resultater"
#| fig-pos: "H"

library(gt)
library(dplyr)

# Beregn andelen signifikante resultater (p-val < 0.05) for hver utvalgsstørrelse
significant_results <- results %>%
  filter(pval < 0.05) %>%
  group_by(n) %>%
  summarise(sig_results = n()/1000)  # Andelen signifikante resultater som desimaltall

# Lag tabellen med gt som viser tallverdien (uten prosent)
significant_results %>%
  gt() %>%
  fmt_number(
    columns = sig_results,
    decimals = 3  # Formaterer til tre desimaler for bedre lesbarhet
  ) %>%
  cols_label(
    sig_results = "Andel Signifikante Resultater (Tall)"
  ) 

```


I denne beregningen skal jeg finne ut hvor mange studier med utvalgsstørelser 8 (lik m1) og 40 (lik m2) som vil få statistisk signifikante resultat med en p-verdi \< 0,05. I @tbl-sig-res ser vi at jeg får resultatene 0,227 for m1, og 0,865 for m2. Dette kan vi gjøre om til prosentverier: m1: 22,7 % og m2: 86,5 %. Det er altså stor forskjell på den betydelige effekten på de forskjellige studiene. I studier med 8 deltakere vil det bare vaære 22,7 % sjanse for a få et statistisk sikgnifikant resultat, mens det i studier med 40 vil være 86,5 % sjanse for å få et statistisk signifikant resultat.

### Oppgave 7.

**Using the pwr package, calculate the power of a one-sample t-test, with a effect size of 1.5/3, your specified significance level and sample sizes 8 and 40. Explain the results in the light of your simulations.**

```{r}
#| echo: false
#| warning: false
#| message: false

# Using the pwr package
library(pwr)

pwr.t.test(n = 8, sig.level = 0.05, d = 1.5/3, type = "one.sample")
pwr.t.test(n = 40, sig.level = 0.05, d = 1.5/3, type = "one.sample")

```

Statistisk styrke er sannsynligheten for å korrekt avvise null-hypotesen, gitt at den alternative hypotesen er sann [@spiegelhalter2019]. Lav statistisk styrke øker sannsynligheten for å begå type I feil som vil si at man feilaktiv avviser en korrekt null-hypotese [@spiegelhalter2019]. Her får m1 en statistisk styrke på 0,232. Dette stemmer bra med simuleringen i oppgave 6. I m2 derimot får vi en statistisk styrke på 0,869 som er en ganske høy statistisk styrke. I studien m2 er det altså lav sannsynlighet for å begå en type I feil. Dette støtter igjen det jeg har kommet fram til i tidligere oppgaver om at større utvalg øker sannsynligheten for et korrekt resultat, og at hvis vi skulle stole på resultatene fra m1 ville vi begå en type II feil.

## Many studies without population effect

### Oppgave 8.

**With a significance level of 5%, how many studies would give you a “false positive” result if you did many repeated studies?**

```{r}
#| echo: false
#| warning: false
#| message: false

population <- rnorm(1000000, mean = 0, sd = 3)


# Create data frames to store the model estimates
results_8 <- data.frame(estimate = rep(NA, 1000), 
                      se = rep(NA, 1000), 
                      pval = rep(NA, 1000), 
                      n = 8)  

results_40 <- data.frame(estimate = rep(NA, 1000), 
                      se = rep(NA, 1000), 
                      pval = rep(NA, 1000), 
                      n = 40)

# A for loop used to sample 1000 studies, each iteration (i) will draw a new sample
# from the population. 

for(i in 1:1000) {
  
  # Draw a sample 
  samp1 <- data.frame(y = sample(population, 8, replace = FALSE))
  samp2 <- data.frame(y = sample(population, 40, replace = FALSE))

  # Model the data
  m1 <- lm(y ~ 1, data = samp1)
  m2 <- lm(y ~ 1, data = samp2)
  
  # Extract values from the models
  results_8[i, 1] <- coef(summary(m1))[1, 1]
  results_8[i, 2] <- coef(summary(m1))[1, 2]
  results_8[i, 3] <- coef(summary(m1))[1, 4]

  results_40[i, 1] <- coef(summary(m2))[1, 1]
  results_40[i, 2] <- coef(summary(m2))[1, 2]
  results_40[i, 3] <- coef(summary(m2))[1, 4]
  
  
}


# Save the results in a combined data frame

results_null <- bind_rows(results_8, results_40)

```



```{r}
#| echo: false
#| warning: false
#| message: false
#| label: tbl-false-pos
#| tbl-cap: "Andel falske positive resultat"
#| fig-pos: "H"

# Calculate number of false positives

false_positives <- results_null %>%
  filter(pval < 0.05) %>%
  group_by(n) %>%
  summarise(sig_results = n() / 1000)

# Lag tabellen med gt som viser tallverdien (uten prosent)
false_positives %>%
  gt() %>%
  fmt_number(
    columns = sig_results,
    decimals = 3  # Formaterer til tre desimaler for bedre lesbarhet
  ) %>%
  cols_label(
    sig_results = "Andel falske positive resultat (Tall)"
  ) 

```

Fra resultatene i simuleringen i @tbl-false-pos kan vi se at vi får det samme resultatet for begge utvalgstørrelsene, nemlig 0,053. Dette vil si at ved å gjennomføre forsøkene gjenntatte ganger i den samme populasjonen vil 5,3 % av forsøkene gi falsk positivt resultat. Dette virker ved første øyenkast rart fordi et større utvalg skal i utgangspunkte redusere sjansen for å få falsk positivt svar fordi man tester en større andel av populajsonen. Men hvis vi gjennomfører det samme forsøket gjenntatte ganger innenfor samme populajson vil vi ende opp med samme resultet gitt at hypotesen stemmer, fordi det også på denne måten vil teste en større andel av populasjonen.

Når vi utfører en statistisk test med signifikansnivå på 0,05 (5%) som i denne simuleringen forventer vi at ca. 5 % av resultatene vil være falske positive. Dette er et naturlig resultat av signifikansnivået vi har satt. Et resultat på 5,3 % er nært nok det satte signifikansnivået til at vi kan konkludere med at testen stemmer. Her er det snakk om repeterte gjennomføringer av det samme forsøkte, så hvis det hadde vært stor forskjell på resultatene som følge av ulikt antall deltakere i utvalgene kunne det ha tydet på at noe var feil med testen eller simuleringen.











